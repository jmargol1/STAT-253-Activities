```{r 03_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

# Overfitting

## Learning Goals {-}

- Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance
- Implement testing and training sets in R using the `tidymodels` package

<br>

Slides from today are available [here](https://docs.google.com/presentation/d/12GOFU2iLGQNhKtBr4OVaP7We2kHasPuJIBBKgDQgIGk/edit?usp=sharing).


<br><br><br>


## The `tidymodels` package {-}


Over this course, we will looking at a broad but linked set of specialized tools applied in statistical machine learning. Specialized tools generally require specialized code. 

Each tool has been developed separately and coded in a unique way. In order to facilitate and streamline the user experience, there have been attempts at creating a uniform interface, such as the `caret` R package.  The developers of the `caret` package are no longer maintaining those packages. They are working on a newer package, called `tidymodels`. 

In this class, we will use the `tidymodels` package, which uses the tidyverse syntax you learned in Stat 155. The `tidymodels` package is a relatively new package and continues to be developed as we speak. This means that I'm learning with you and in a month or two, there may be improved functionality.  


As I introduced in the R code videos, we have a general workflow structure that includes a model specification and a recipe (formula + preprocessing steps). 

```{r eval=FALSE}
# Load the package
library(tidymodels)
tidymodels_prefer()

# Set the seed for the random number generator
set.seed(123)

# Specify Model
model_spec <-
    linear_reg() %>% # type of model
    set_engine(engine = __) %>% # algorithm to fit the model
    set_args(__) %>% # hyperparameters/tuning parameters are needed for some models
    set_mode(__) # regression or classification
    
# Specify Recipe (if you have preprocessing steps)
rec <- recipe(formula, data) %>%
  step_{FILLIN}() %>%
  step_{FILLIN}() 

# Create Workflow (Model + Recipe)
model_wf <- workflow() %>%
    add_recipe(rec) %>% #or add_formula()
    add_model(model_spec)
```

We can fit that workflow to training data.

```{r eval=FALSE}
# Fit Model to training data (without a recipe)
fit_model <- fit(model_spec, formula, data_train)

# Fit Model & Recipe to training data
fit_model <- fit(model_wf, data_train)
```

And then we can evaluate that fit model on testing data (new data that has not been used to fit the model).

```{r}
# Evaluate on testing data
model_output <- fit_model %>%
  predict(new_data = data_test) %>% # this function will apply recipe to new_data and do prediction
  bind_cols(data_test)
  
reg_metrics <- metric_set(rmse, rsq, mae)

model_output %>%
  reg_metrics(truth = __, estimate = .pred)
```



<br>

The power of `tidymodels` is that it allows us to streamline the vast world of machine learning techniques into one common syntax. On top of `"lm"`, there are many other different machine learning methods that we can use.

<br>

In the exercises below, youâ€™ll need to adapt the code above to fit a linear regression model (`engine = "lm"`).



<br><br><br>


## Exercises {-}

**You can download a template RMarkdown file to start from [here](template_rmds/03-overfitting.Rmd).**

### Context {-}

We'll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements.

- `fatBrozek`: Percent body fat using Brozek's equation: 457/Density - 414.2
- `fatSiri`: Percent body fat using Siri's equation: 495/Density - 450
- `density`: Density determined from underwater weighing (gm/cm^3).
- `age`: Age (years)
- `weight`: Weight (lbs)
- `height`: Height (inches)
- `neck`: Neck circumference (cm)
- `chest`: Chest circumference (cm)
- `abdomen`: Abdomen circumference (cm)
- `hip`: Hip circumference (cm)
- `thigh`: Thigh circumference (cm)
- `knee`: Knee circumference (cm)
- `ankle`: Ankle circumference (cm)
- `biceps`: Biceps (extended) circumference (cm)
- `forearm`: Forearm circumference (cm)
- `wrist`: Wrist circumference (cm)

It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for `fatSiri` using just circumference measurements, which are more easily attainable. (We won't use `fatBrozek` or `density` as predictors because they're other outcome variables.)

```{r}
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
tidymodels_prefer()

bodyfat_train <- read_csv("https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1")

# Remove the fatBrozek and density variables (and one variable that a replicate of others)
bodyfat_train <- bodyfat_train %>%
    select(-fatBrozek, -density, -hipin)
```

### Exercise 1: 5 models {-}

Consider the 5 models below:

```{r}
lm_spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

mod1 <- fit(lm_spec,
            fatSiri ~ age+weight+neck+abdomen+thigh+forearm, 
            data = bodyfat_train)
#Mod 1 is creating a linear regression model for fatSiri including age, weight, neck, abdomen, thigh, and forearm sizes

mod2 <- fit(lm_spec,
            fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, 
            data = bodyfat_train)
#Mod 2 is creating a linear regression model for fatSiri including age, weight, neck, abdomen, thigh, and forearm, biceps, chest , and hips

mod3 <- fit(lm_spec,
            fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, 
            data = bodyfat_train)

mod4 <- fit(lm_spec,
            fatSiri ~ .,  # The . means all predictors
            data = bodyfat_train) 

bf_recipe <- recipe(fatSiri ~ .,  data = bodyfat_train) %>%
  step_normalize(all_numeric_predictors())

bf_wf <- workflow() %>%
  add_recipe(bf_recipe) %>%
  add_model(lm_spec) 

mod5 <- fit(bf_wf,
            data = bodyfat_train) 
```

a. **STAT 155 review:** Look at the `tidy()` of `mod1`. Contextually interpret the coefficient for the weight predictor. Is anything surprising? Why might this be?

```{r}
tidy(mod1)
```
Weight shows an odd result where the heavier someone is, it seems the less fat they would be which seems contradictory

b. Explain how mod5 is different than mod4. You may want to look at `bf_recipe %>% prep(bodyfat_train) %>% juice()` to see the preprocessed training data. 

```{r}
bf_recipe %>% 
  prep(bodyfat_train) %>% 
  juice()
```
Model 5 used a workflow and recipe to normalize all the data into standard deviation units as opposed to having some units in years and some in centimeters or inches.

c. Which model will have the lowest training RMSE, and why? Explain before calculating (that is part d). 

Model 5 because it runs all of the variables in the equation while also having standard deviation values smaller than the normal units.

d. Compute the training RMSE for models 1 through 5 to check your answer for part c. Write a sentence interpreting one of values of RMSE in context. 
```{r}
mod1_output <- mod1 %>%
    predict(new_data = bodyfat_train) %>%
  bind_cols(bodyfat_train)

mod2_output <- mod2 %>%
    predict(new_data = bodyfat_train) %>%
  bind_cols(bodyfat_train)

mod3_output <- mod3 %>%
    predict(new_data = bodyfat_train) %>%
  bind_cols(bodyfat_train)

mod4_output <- mod4 %>%
    predict(new_data = bodyfat_train) %>%
  bind_cols(bodyfat_train)

mod5_output <- mod5 %>%
    predict(new_data = bodyfat_train) %>%
  bind_cols(bodyfat_train)

mod1_output %>%
    rmse(truth = fatSiri, estimate = .pred)

mod2_output %>%
    rmse(truth = fatSiri, estimate = .pred)

mod3_output %>%
    rmse(truth = fatSiri, estimate = .pred)

mod4_output %>%
    rmse(truth = fatSiri, estimate = .pred)

mod5_output %>%
    rmse(truth = fatSiri, estimate = .pred)
```

e. Which model do you think is the "best"? You may calculate MAE and R squared as well to justify your answer.

I think model 2 is the best because there is not much difference between model 2 and 3 in rmse but there 

f. Which model do you think will perform worst on new test data? Why?

Models 4 and 5 will struggle on a new dataset because those models use too many predictors that are too specific to the original data set, therefore a new dataset may get very different results



# Lets make RMSE Easier to do!!!!
```{r}
train_rmse <- function(mod) {
  mod_output <- mod %>%
    predict(new_data = bodyfat_train) %>%
    bind_cols(bodyfat_train)
  mod_output %>%
    rmse(truth = fatSiri, estimate = .pred)
}
train_rmse(mod1)
train_rmse(mod2)
train_rmse(mod3)
train_rmse(mod4)
train_rmse(mod5)
```
# Add MAE and Rsquared
```{r}
train_metrics <- function(mod) {
  mod_output <- mod %>%
    predict(new_data = bodyfat_train) %>%
    bind_cols(bodyfat_train)
  reg_metrics <- metric_set(rmse, rsq, mae)
  mod_output %>% reg_metrics(truth = fatSiri, estimate = .pred)
}

train_metrics(mod1)
train_metrics(mod2)
train_metrics(mod3)
train_metrics(mod4)
train_metrics(mod5)
```





### Exercise 2: Visualizing Predictions {-}


a. Sequentially run the code below, ending before pipe, comma, or +. For each row of code below, discuss what it does. Add comments to the end of the line after the pipe (with # in front) to explain what each line does.

```{r}
mod5 %>% #pipes model 5 into this funtion
  tidy() %>% #Shows coefficients of mod 5 variables
  slice(-1) %>% #Removed the intercept from the equation 
  mutate(lower = estimate - 1.96*std.error, upper = estimate + 1.96*std.error) %>% #Calculates a confidence interval for each of the variables
  ggplot() + # Creates the frame for a plot
    geom_vline(xintercept=0, linetype=4) + # Adds a vertical line at x = 0
    geom_point(aes(x=estimate, y=term)) + # Adds the slope estimate for each variable's standard deviations as a dot on the plot
    geom_segment(aes(y=term, yend=term, x=lower, xend=upper), arrow = arrow(angle=90, ends='both', length = unit(0.1, 'cm'))) + # Adds bars displaying confidence intervals of slope estimates for standard deviations of each variable
    labs(x = 'Coefficient estimate (95% CI)', y = 'Feature') +  #Gave better labels
    theme_classic() #removed the grey background
```

b. Sequentially run each line of code (below), end before pipe or comma. For each row of code below, discuss what it does. Add comments to the end of the line after the pipe (with # in front) to explain what each line does.

```{r}
bodyfat_train %>% # piped in the bodyfat_train data
  mutate(id = row_number()) %>% # made a new variable number the rows
  pivot_longer(-c(fatSiri, id), names_to = 'key', values_to = 'value') %>% # Gave the variable values for the first person in the groups next to their fatsiri number
  right_join(  # 
    (mod4 %>%  tidy() %>% slice(-1) %>% select(term, estimate)), # 
    by = c('key'='term')
  ) %>% #right_join finishes here
  mutate(effect = value * estimate) %>% # 
  ggplot(aes(x = effect, y = key)) + # 
    geom_boxplot() + # 
    geom_vline(xintercept = 0, color = 'grey') + # 
    labs(x = 'Effect/Contribution to Predicted BodyFat Percent', y = 'Feature') +  # 
    theme_classic() #
```


### Exercise 3: Evaluating on Test Data {-}

Now that you've thought about how well the models might perform on test data, deploy it in the real world by applying it to a new set of 172 adult males. You'll need to update the `new_data` to use `bodyfat_test` instead of `bodyfat_train`.

```{r}
bodyfat_test <- read_csv("https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1")
```

```{r}
# Use fit/trained models and evaluate on test data

```


a. Calculate the test RMSE, MAE, and R squared for all five of the models. 

b. Look back to Exercise 1 and see which model you thought was "best" based on the training data. Is that the "best" model in terms of predicting on new data? Explain.

c. In "real life" we only have one data set. To get a sense of predictive performance on new test data, we could split our data into two groups. Discuss pros and cons of ways you might split the data. How big should the training set be? How big should the testing set be?


### Exercise 4: Overfitting {-}

If you have time, consider the following relationship. Imagine a set of predictions that is overfit to this training data. You are not limited to lines. Draw a picture of that function of predictions on a piece of paper.

```{r eval=TRUE}
set.seed(123)

data_train <- tibble(
  x = runif(15,0,7),
  y = x^2 + rnorm(15,sd = 7)
)
data_train %>%
  ggplot(aes(x = x, y = y)) + 
  geom_point() +
  theme_classic()
```

